# Pimaindean Dataset Analutics

- [dataset](https://www.kaggle.com/uciml/pima-indians-diabetes-database)

# ライブラリ読み込み

```{r}
# dataframe操作パッケージ
library(dplyr)
# 欠損値の補完パッケージ
library(ForImp)
# train_test_split/dummyVarsを行うパッケージ
library(caret)
# Kerasを使うパッケージ
library(keras)
```


# データセット読み込み
```{r}
df <- read.csv('diabetes.csv', header = TRUE)
head(df, n = 5)
```

# データ概要の確認

```{r}
summary(df)
```

```{r}
print(nrow(df)) # 行
print(ncol(df)) # 列
```

```{r}
colnames(df)
```

```{r}
na_df <- apply(df, 2, function(y) any(is.na(y)))
na_df
```

```{r}
null_df <- sapply(df, function(y) any(is.null(y)))
null_df
```

# 前処理 

## データの分割
```{r}
X = select(.data = df, !Outcome)
y = df$Outcome
```

```{r}
set.seed(2020)
# test：分割したレコード（今回は20%）の行番号を受け取る
# データの分布を維持したまま分割
test <- createDataPartition(df$Outcome, p=0.2, list=FALSE)
X_train <- X[-test, ]
y_train <- y[-test]
X_test <- X[test, ]
y_test <- y[test]
```

```{r}
X_train
```

## 数値カラムとカテゴリ変数カラムに分割
```{r}
numerical_features = c('Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age')
categolical_features = c(NULL)
```

## 数値カラムに欠損値処理(中央値で穴埋め)・標準化
```{r}
# 中央値で補完
medianimp(X_train[, numerical_features])
medianimp(X_test[, numerical_features])
# 標準化
scale(X_train[, numerical_features])
scale(X_test[, numerical_features])
```


# カテゴリ変数カラムに欠損値処理（missingで穴埋め）・OneHot処理

```{r}
## missingで補完
# X_train[is.na(X_train[, categolical_features])] <- 'missing'
# X_train[is.na(X_train[, categolical_features])] <- 'missing'
# X_test[is.null(X_test[, categolical_features])] <- 'missing'
# X_test[is.null(X_test[, categolical_features])] <- 'missing'
## One-Hot Encoding
# dummy_model <- dummyVars(~categolical_features, data=X_train, fullRank=FALSE)
# dummy_vars <- predict(dummy_model, X_train)
# X_train <- data.frame(X_train, dummy_vars)
```



# KerasでDNNモデル作成

```{r}
clf_keras = keras_model_sequential()
# 重みの初期化方法　→　ReLUのときは…
clf_keras %>%
  layer_dense(units=32,activation="relu",input_shape = 8, kernel_initializer = 'glorot_normal') %>%
  layer_dropout(rate=0.1) %>%
  layer_dense(units=32,activation="relu",input_shape = 32, kernel_initializer = 'glorot_normal') %>%
  layer_dropout(rate=0.1) %>%
  layer_dense(units=8,activation="relu",input_shape = 16, kernel_initializer = 'glorot_normal') %>%
  layer_dense(units=4,activation="relu",input_shape = 8, kernel_initializer = 'glorot_normal') %>%
  layer_dense(units=1,activation="sigmoid",input_shape = 4, kernel_initializer = 'glorot_normal')
clf_keras %>%
  compile(loss = 'loss_binary_crossentropy', optimizer = optimizer_rmsprop(), metrics = c("accuracy"))
clf_keras %>%
  fit(X_train, y_train, epochs = 5, batch_size = 5, validation_split = 0.2, verbose = TRUE)
```

